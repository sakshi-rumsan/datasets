{"cells":[{"cell_type":"markdown","metadata":{"id":"6_rIQRqC-ZzS"},"source":["Generate datasets using local llm in ollama"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bOl7W6ft98cx"},"outputs":[],"source":["import os\n","import pandas as pd\n","from langchain.llms import Ollama\n","\n","model = os.environ.get(\"MODEL\", \"mistral\")\n","callbacks = []\n","\n","llm = Ollama(model=model, callbacks=callbacks)\n","\n","csv_file_path = 'total_2793_dataset_processed.csv.csv'\n","output_file_path = 'processed_Datasets.csv'\n","\n","chunk_size = 100\n","\n","total_rows = sum(1 for row in pd.read_csv(csv_file_path)) - 1\n","\n","processed_rows = 0\n","for chunk in pd.read_csv(csv_file_path, chunksize=chunk_size):\n","    results = []\n","    for index, row in chunk.iterrows():\n","        user_content = row['user_content']\n","        query = f\"Generate only three similar questions based on the original context: {user_content}\"\n","        answer = llm(query)\n","        assistant = row['assistant_content']\n","        is_liked = row['is_liked']\n","        results.append({\n","            \"user_content\": answer,\n","            \"assistant_content\": assistant,\n","            \"is_liked\": is_liked\n","        })\n","        print(answer, end=\" \")\n","\n","    results_df = pd.DataFrame(results)\n","    results_df.to_csv(output_file_path, mode='a', index=False, header=False)\n","\n","    processed_rows += len(chunk)\n","    progress = (processed_rows / total_rows) * 100\n","    print(f\"Progress: [{'#' * int(progress / 2)}{' ' * (50 - int(progress / 2))}] {progress:.2f}% complete\", end='\\r')\n","\n","print(f\"\\nQuestion generation completed and saved to '{output_file_path}'.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NlmDrkDICjK7"},"outputs":[],"source":["#use hosted ollama but may cause disturbance due to over traffic,\n","#huggingface"]},{"cell_type":"markdown","metadata":{"id":"f3pD3_88DYTc"},"source":["can be written either to generate or rewrite."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dWuCDxmsDYCW"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DdrOC_QgCZtP"},"outputs":[],"source":["\n","!pip install transformers pandas\n","\n","import pandas as pd\n","from transformers import pipeline\n","\n","file_path = 'total_2793_dataset_processed.csv'\n","dataset = pd.read_csv(file_path)\n","\n","generator = pipeline(\"text-generation\", model=\"meta-llama/Meta-Llama-3-8B-Instruct\", max_length=200)\n","\n","def clean_text(text, generator):\n","\n","    prompt = f\"Rewrite the following text to remove unnecessary phrases and refine the content: {text}\"\n","    response = generator(prompt, num_return_sequences=1)[0]['generated_text']\n","\n","\n","    cleaned_text = response.strip()\n","\n","    return cleaned_text\n","\n","for index in dataset.index:\n","    user_text = dataset.at[index, 'user']\n","    assistant_text = dataset.at[index, 'assistant']\n","\n","    dataset.at[index, 'user'] = clean_text(user_text, generator)\n","    dataset.at[index, 'assistant'] = clean_text(assistant_text, generator)\n","\n","cleaned_file_path = 'total_2793_dataset_cleaned.csv'\n","dataset.to_csv(cleaned_file_path, index=False)\n","print(f\"Cleaned dataset saved to {cleaned_file_path}.\")"]},{"cell_type":"markdown","metadata":{"id":"G39UWoc1DIlU"},"source":["filling null values by sentimenntal annalysis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9joiKoKoDHXs"},"outputs":[],"source":["import pandas as pd\n","from transformers import pipeline, AutoTokenizer\n","\n","file_path = 'merged_dataset.csv'\n","df = pd.read_csv(file_path)\n","\n","sentiment_pipeline = pipeline('sentiment-analysis')\n","tokenizer = AutoTokenizer.from_pretrained(sentiment_pipeline.model.config._name_or_path)\n","\n","def analyze_sentiment(content):\n","    tokens = tokenizer.encode(content, truncation=True, max_length=510)\n","    content_truncated = tokenizer.decode(tokens)\n","\n","    result = sentiment_pipeline(content_truncated)[0]\n","    return 1 if result['label'] == 'POSITIVE' else 0\n","\n","df['combined_content'] = df['user_content'] + ' ' + df['assistant_content']\n","\n","df['sentiment_score'] = df['combined_content'].apply(analyze_sentiment)\n","\n","df['is_liked'] = df.apply(lambda row: row['sentiment_score'] if pd.isnull(row['is_liked']) else row['is_liked'], axis=1)\n","\n","df['is_liked'] = df['is_liked'].map({1: 'true', 0: 'false', 'true': 'true', 'false': 'false'})\n","\n","df.drop(columns=['combined_content', 'sentiment_score'], inplace=True)\n","\n","df.to_csv(file_path, index=False)\n","\n","print(\"Sentiment analysis completed and 'is_liked' column updated for null values.\")\n"]},{"cell_type":"markdown","metadata":{"id":"uIzwufr4_svJ"},"source":["text preprocessing: tokenization,removing stop words,ing words, lemmatization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qqkA3Iip_jBo"},"outputs":[],"source":["import pandas as pd\n","import re\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","file_path = 'total_2793_dataset - total_dataset.csv.csv'\n","dataset = pd.read_csv(file_path)\n","\n","print(dataset.head())\n","print(dataset.columns)\n","\n","lemmatizer = WordNetLemmatizer()\n","\n","def preprocess_text(text):\n","    text = text.lower()\n","    text = re.sub(r'[^a-z\\s]', '', text)\n","    text = re.sub(r'\\d+', '', text)\n","    text= re.sub(r'\\b\\w+ing\\b', '', text)\n","    tokens = word_tokenize(text)\n","    stop_words = set(stopwords.words('english'))\n","    tokens = [word for word in tokens if word not in stop_words]\n","    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n","    processed_text = ' '.join(tokens)\n","    return processed_text\n","\n","dataset['user'] = dataset['user'].apply(preprocess_text)\n","dataset['assistant'] = dataset['assistant'].apply(preprocess_text)\n","\n","print(dataset[['user_processed', 'assistant_processed']].head())\n","\n","processed_file_path = 'total_2793_dataset_processed.csv'\n","dataset.to_csv(processed_file_path, index=False)\n","\n","print(f\"Processed data saved to {processed_file_path}\")\n"]},{"cell_type":"markdown","metadata":{"id":"Ox6iK0VAAI1Y"},"source":["checking the rows with noise using huggign face text -classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O76vbsQgAG8v"},"outputs":[],"source":["import pandas as pd\n","from transformers import pipeline\n","\n","file_path = 'total_2793_dataset_processed.csv'\n","dataset = pd.read_csv(file_path)\n","\n","classifier = pipeline(\"text-classification\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n","\n","def identify_noise(text, classifier):\n","    result = classifier(text)[0]\n","    return result['label'] == 'NEGATIVE' and result['score'] \u003e 0.8\n","\n","noisy_rows = []\n","\n","for index, row in dataset.iterrows():\n","    user_text = row['user']\n","    assistant_text = row['assistant']\n","\n","    user_noise = identify_noise(user_text, classifier)\n","    assistant_noise = identify_noise(assistant_text, classifier)\n","\n","    if user_noise or assistant_noise:\n","        noisy_rows.append(index)\n","\n","if noisy_rows:\n","    print(f\"Noisy rows found at indices: {noisy_rows}\")\n","else:\n","    print(\"No noise detected in the dataset.\")\n"]},{"cell_type":"markdown","metadata":{"id":"HESHsGmIAokS"},"source":["listing and removing words which holds no significant meaning using text classifier . removing also meaningless words in a row accordinng to the context\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"inLvWpM5ARhu"},"outputs":[],"source":["import pandas as pd\n","from transformers import pipeline\n","\n","dataset = pd.read_csv('total_2793_dataset_processed.csv')\n","\n","classifier = pipeline(\"text-classification\", model=\"distilbert-base-uncased\")\n","\n","def remove_noise(text):\n","    predictions = classifier(text)\n","    return ' '.join([word for word, pred in zip(text.split(), predictions) if pred['label'] != 'LABEL_FOR_NOISE'])\n","\n","dataset['user_cleaned'] = dataset['user'].apply(remove_noise)\n","dataset['assistant_cleaned'] = dataset['assistant'].apply(remove_noise)\n","\n","dataset.to_csv('cleaned_dataset.csv', index=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FzZHz1znAyle"},"outputs":[],"source":["import pandas as pd\n","\n","df = pd.read_csv('cleaned_dataset.csv')\n","\n","def remove_cleaned_words(text, cleaned_words):\n","    for word in str(cleaned_words).split():\n","        text = text.replace(word, '')\n","    return text.strip()\n","\n","df['user'] = df.apply(lambda row: remove_cleaned_words(row['user'], row['user_cleaned']), axis=1)\n","df['assistant'] = df.apply(lambda row: remove_cleaned_words(row['assistant'], row['assistant_cleaned']), axis=1)\n","\n","df.to_csv('your_file_cleaned.csv', index=False)\n","\n","print(\"Processing completed and file saved as 'your_file_cleaned.csv'.\")\n"]},{"cell_type":"markdown","metadata":{"id":"fllJyDX2BnDo"},"source":["cross checking noise"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k7nokm70BCos"},"outputs":[],"source":["import pandas as pd\n","from transformers import pipeline\n","\n","file_path = 'total_2793_dataset_processed.csv'\n","dataset = pd.read_csv(file_path)\n","\n","classifier = pipeline(\"text-classification\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n","\n","def identify_noise(text, classifier):\n","    result = classifier(text)[0]\n","    return result['label'] == 'NEGATIVE' and result['score'] \u003e 0.8\n","\n","noisy_rows = []\n","\n","for index, row in dataset.iterrows():\n","    user_text = row['user']\n","    assistant_text = row['assistant']\n","\n","    user_noise = identify_noise(user_text, classifier)\n","    assistant_noise = identify_noise(assistant_text, classifier)\n","\n","    if user_noise or assistant_noise:\n","        noisy_rows.append(index)\n","\n","if noisy_rows:\n","    print(f\"Noisy rows found at indices: {noisy_rows}\")\n","else:\n","    print(\"No noise detected in the dataset.\")\n"]},{"cell_type":"markdown","metadata":{"id":"HE9nyq93B5S9"},"source":["removing only noise words not droping row"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1vxBeMg8BuDK"},"outputs":[],"source":["import pandas as pd\n","from transformers import pipeline\n","\n","file_path = 'your_file_cleaned.csv'\n","dataset = pd.read_csv(file_path)\n","\n","classifier = pipeline(\"text-classification\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n","\n","def identify_noise(word, classifier):\n","    result = classifier(word)[0]\n","    return result['label'] == 'NEGATIVE' and result['score'] \u003e 0.8\n","\n","def clean_sentence(sentence, classifier):\n","    if not isinstance(sentence, str):\n","        return sentence\n","\n","    words = sentence.split()\n","    cleaned_words = [word for word in words if not identify_noise(word, classifier)]\n","    return ' '.join(cleaned_words)\n","\n","dataset['user'] = dataset['user'].apply(lambda x: clean_sentence(x, classifier))\n","dataset['assistant'] = dataset['assistant'].apply(lambda x: clean_sentence(x, classifier))\n","\n","cleaned_file_path = 'your_file_cleaned_no_noise.csv'\n","dataset.to_csv(cleaned_file_path, index=False)\n","\n","print(f\"Dataset cleaned. Cleaned dataset saved to {cleaned_file_path}.\")\n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2256,"status":"ok","timestamp":1723223001617,"user":{"displayName":"Sakshi Nepal","userId":"06409844641326949268"},"user_tz":-345},"id":"xLopcFqoF1z4","outputId":"92d2a76c-6978-40a8-e6c5-2720fa58d980"},"outputs":[{"name":"stdout","output_type":"stream","text":["First 5 rows of the dataset:\n","                                                user  \\\n","0       product inventory operation attribute reason   \n","1           attribute mef standard product inventory   \n","2  table product inventory attribute mef name des...   \n","3  id unique identifier organization customer tra...   \n","4           detail requirement product inventory api   \n","\n","                                           assistant  is_liked is_duplicated  \\\n","0  correct mention product inventory operation at...     False            no   \n","1  mef metro ethernet forum standard organization...     False            no   \n","2  table product inventory attribute mentioned me...     False            no   \n","3  previous complete table attribute table mef at...      True            no   \n","4  mef standard provides set requirement guidelin...     False            no   \n","\n","  user_cleaned assistant_cleaned  \n","0      omitted         apologize  \n","1         main  introductionnthe  \n","2       create            sample  \n","3        buyer         apologize  \n","4          mef             thank  \n","\n","Dataset Info:\n","\u003cclass 'pandas.core.frame.DataFrame'\u003e\n","RangeIndex: 2792 entries, 0 to 2791\n","Data columns (total 6 columns):\n"," #   Column             Non-Null Count  Dtype \n","---  ------             --------------  ----- \n"," 0   user               2768 non-null   object\n"," 1   assistant          2792 non-null   object\n"," 2   is_liked           2792 non-null   bool  \n"," 3   is_duplicated      2792 non-null   object\n"," 4   user_cleaned       2792 non-null   object\n"," 5   assistant_cleaned  2792 non-null   object\n","dtypes: bool(1), object(5)\n","memory usage: 111.9+ KB\n","None\n","\n","Summary Statistics:\n","                                                     user  \\\n","count                                                2768   \n","unique                                               2688   \n","top     context aligned mef standard product specifica...   \n","freq                                                    6   \n","\n","                                                assistant is_liked  \\\n","count                                                2792     2792   \n","unique                                                151        2   \n","top     previous complete table attribute table mef at...     True   \n","freq                                                   70     1588   \n","\n","       is_duplicated user_cleaned assistant_cleaned  \n","count           2792         2792              2792  \n","unique             2          497                29  \n","top              yes        could         apologize  \n","freq            2600          242               788  \n","\n","Missing Values:\n","user                 24\n","assistant             0\n","is_liked              0\n","is_duplicated         0\n","user_cleaned          0\n","assistant_cleaned     0\n","dtype: int64\n"]}],"source":["import pandas as pd\n","\n","# Load the dataset\n","df = pd.read_csv('your_file_cleaned_no_noise.csv')\n","\n","# Display the first few rows of the dataset\n","print(\"First 5 rows of the dataset:\")\n","print(df.head())\n","\n","# Display dataset information (columns, data types, non-null counts)\n","print(\"\\nDataset Info:\")\n","print(df.info())\n","\n","# Display summary statistics\n","print(\"\\nSummary Statistics:\")\n","print(df.describe(include='all'))\n","\n","# Check for missing values\n","print(\"\\nMissing Values:\")\n","print(df.isnull().sum())\n"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":470,"status":"ok","timestamp":1723223140993,"user":{"displayName":"Sakshi Nepal","userId":"06409844641326949268"},"user_tz":-345},"id":"yh0ROHT5Gb3_"},"outputs":[],"source":["import pandas as pd\n","\n","# Load the dataset\n","df = pd.read_csv('your_file_cleaned_no_noise.csv')\n","\n","# Drop rows with missing values\n","df_cleaned = df.dropna()\n","\n","# Save the cleaned DataFrame to a new CSV file\n","df_cleaned.to_csv('your_file_cleaned_no_missing1.csv', index=False)\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6759,"status":"ok","timestamp":1723223459435,"user":{"displayName":"Sakshi Nepal","userId":"06409844641326949268"},"user_tz":-345},"id":"puyDGYbYHPMm","outputId":"d9ded5c1-7ad2-4878-f21c-d5d3995cc514"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.20.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n","Requirement already satisfied: huggingface-hub\u003c1.0,\u003e=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n","Requirement already satisfied: numpy\u003c2.0,\u003e=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml\u003e=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: safetensors\u003e=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n","Requirement already satisfied: tokenizers\u003c0.20,\u003e=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: tqdm\u003e=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n","Requirement already satisfied: pyarrow\u003e=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n","Requirement already satisfied: dill\u003c0.3.9,\u003e=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec\u003c=2024.5.0,\u003e=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]\u003c=2024.5.0,\u003e=2023.1.0-\u003edatasets) (2024.5.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.1)\n","Requirement already satisfied: typing-extensions\u003e=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107-\u003etorch) (12.6.20)\n","Requirement already satisfied: aiohappyeyeballs\u003e=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-\u003edatasets) (2.3.4)\n","Requirement already satisfied: aiosignal\u003e=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp-\u003edatasets) (1.3.1)\n","Requirement already satisfied: attrs\u003e=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-\u003edatasets) (24.2.0)\n","Requirement already satisfied: frozenlist\u003e=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp-\u003edatasets) (1.4.1)\n","Requirement already satisfied: multidict\u003c7.0,\u003e=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp-\u003edatasets) (6.0.5)\n","Requirement already satisfied: yarl\u003c2.0,\u003e=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-\u003edatasets) (1.9.4)\n","Requirement already satisfied: async-timeout\u003c5.0,\u003e=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-\u003edatasets) (4.0.3)\n","Requirement already satisfied: charset-normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers) (3.3.2)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers) (3.7)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers) (2.0.7)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers) (2024.7.4)\n","Requirement already satisfied: MarkupSafe\u003e=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-\u003etorch) (2.1.5)\n","Requirement already satisfied: python-dateutil\u003e=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas-\u003edatasets) (2.8.2)\n","Requirement already satisfied: pytz\u003e=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas-\u003edatasets) (2024.1)\n","Requirement already satisfied: tzdata\u003e=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas-\u003edatasets) (2024.1)\n","Requirement already satisfied: mpmath\u003c1.4,\u003e=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy-\u003etorch) (1.3.0)\n","Requirement already satisfied: six\u003e=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil\u003e=2.8.2-\u003epandas-\u003edatasets) (1.16.0)\n"]}],"source":["!pip install transformers datasets torch\n"]},{"cell_type":"markdown","metadata":{"id":"XJVOfHDJdBod"},"source":["genneration"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":289},"id":"UUU6rOT4GrMs"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"48396c6c620e49f5b4aed02682c89180","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/2214 [00:00\u003c?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4016: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"179dd406572046b98077e0e9f6c01852","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/554 [00:00\u003c?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    \u003cdiv\u003e\n","      \n","      \u003cprogress value='256' max='831' style='width:300px; height:20px; vertical-align: middle;'\u003e\u003c/progress\u003e\n","      [256/831 2:58:21 \u003c 6:43:44, 0.02 it/s, Epoch 0.92/3]\n","    \u003c/div\u003e\n","    \u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n"," \u003ctr style=\"text-align: left;\"\u003e\n","      \u003cth\u003eEpoch\u003c/th\u003e\n","      \u003cth\u003eTraining Loss\u003c/th\u003e\n","      \u003cth\u003eValidation Loss\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\u003cp\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"}],"source":["import pandas as pd\n","from datasets import Dataset\n","from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments\n","\n","df = pd.read_csv('your_file_cleaned_no_missing1.csv')\n","df.rename(columns={'user': 'input_text',\n","                   'assistant': 'target_text'}, inplace=True)\n","df.dropna(subset=['input_text', 'target_text'], inplace=True)\n","\n","dataset = Dataset.from_pandas(df)\n","train_test_split = dataset.train_test_split(test_size=0.2)\n","train_dataset = train_test_split['train']\n","eval_dataset = train_test_split['test']\n","\n","tokenizer = AutoTokenizer.from_pretrained('t5-small')\n","model = AutoModelForSeq2SeqLM.from_pretrained('t5-small')\n","\n","def preprocess_function(examples):\n","    inputs = examples['input_text']\n","    targets = examples['target_text']\n","    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding='max_length') # Add padding here\n","    with tokenizer.as_target_tokenizer():\n","        labels = tokenizer(targets, max_length=512, truncation=True, padding='max_length') # Add padding here\n","    model_inputs['labels'] = labels['input_ids']\n","    return model_inputs\n","\n","train_dataset = train_dataset.map(preprocess_function, batched=True)\n","eval_dataset = eval_dataset.map(preprocess_function, batched=True)\n","\n","training_args = TrainingArguments(\n","    output_dir='./results',\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    logging_dir='./logs',\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=8,\n","    num_train_epochs=3,\n","    weight_decay=0.01,\n","    load_best_model_at_end=True,\n","    metric_for_best_model='accuracy',\n","    save_total_limit=2,\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset,\n","    tokenizer=tokenizer,\n",")\n","\n","trainer.train()\n","\n","metrics = trainer.evaluate()\n","print(metrics)\n","\n","model.save_pretrained('./trained_model')\n","tokenizer.save_pretrained('./trained_model')\n","\n","question = \"Can you explain the rationale behind leaving out the Product Inventory Operation Attributes?\"\n","input_ids = tokenizer.encode(question, return_tensors='pt')\n","outputs = model.generate(input_ids)\n","response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","print(\"Generated Response:\", response)"]},{"cell_type":"markdown","metadata":{"id":"Oq28WD46cZpl"},"source":["if immediate result is expected : less time evaluation\n"]},{"cell_type":"markdown","metadata":{"id":"KmCJD1ZBdDNJ"},"source":["Classification"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bkedJimjcYH7"},"outputs":[],"source":["!pip install transformers pandas scikit-learn datasets\n","\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n","from transformers import Trainer, TrainingArguments\n","from datasets import Dataset\n","\n","file_path = 'your_file_cleaned_no_noise.csv'\n","data = pd.read_csv(file_path)\n","\n","print(data.head())\n","print(data.info())\n","\n","aggregated_data = data.groupby(['assistant', 'is_liked'])['user'].apply(lambda x: ' '.join(x.astype(str))).reset_index()\n","\n","model_name = \"bert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForSequenceClassification.from_pretrained(model_name)\n","classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n","\n","def evaluate_model(data, classifier, label_column='is_liked', text_column='assistant'):\n","    data = data.dropna(subset=[label_column])\n","    texts = data[text_column].tolist()\n","    labels = data[label_column].apply(lambda x: 1 if x == 'True' else 0).tolist()\n","\n","    predictions = []\n","    for text in texts:\n","        result = classifier(text)\n","        predictions.append(1 if result[0]['label'] == 'LABEL_1' else 0)\n","\n","    accuracy = accuracy_score(labels, predictions)\n","    return accuracy\n","\n","pretrained_accuracy = evaluate_model(aggregated_data, classifier, label_column='is_liked', text_column='assistant')\n","print(f\"Pretrained Model Accuracy: {pretrained_accuracy}\")\n","\n","train_data, test_data = train_test_split(aggregated_data.dropna(subset=['is_liked']), test_size=0.2, random_state=42)\n","\n","def train_custom_model(train_data, model_name=\"bert-base-uncased\"):\n","    tokenizer = AutoTokenizer.from_pretrained(model_name)\n","    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n","\n","    train_texts = train_data['assistant'].tolist()\n","    train_labels = train_data['is_liked'].apply(lambda x: 1 if x == 'True' else 0).tolist()\n","    train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n","    train_dataset = Dataset.from_dict({'input_ids': train_encodings['input_ids'], 'attention_mask': train_encodings['attention_mask'], 'labels': train_labels})\n","\n","    training_args = TrainingArguments(\n","        output_dir='./results',\n","        num_train_epochs=3,\n","        per_device_train_batch_size=8,\n","        warmup_steps=500,\n","        weight_decay=0.01,\n","        logging_dir='./logs',\n","    )\n","\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_dataset,\n","    )\n","\n","    trainer.train()\n","    return model\n","\n","custom_model = train_custom_model(train_data)\n","\n","custom_classifier = pipeline(\"text-classification\", model=custom_model, tokenizer=tokenizer)\n","custom_model_accuracy = evaluate_model(test_data, custom_classifier, label_column='is_liked', text_column='assistant')\n","print(f\"Custom Model Accuracy: {custom_model_accuracy}\")\n","\n","def minimum_data_for_accuracy(data, classifier, target_accuracy=0.8, label_column='is_liked', text_column='assistant'):\n","    for i in range(1, len(data)+1):\n","        subset = data.sample(n=i, random_state=42)\n","        accuracy = evaluate_model(subset, classifier, label_column=label_column, text_column=text_column)\n","        if accuracy \u003e= target_accuracy:\n","            return i, accuracy\n","    return len(data), accuracy\n","\n","min_data_needed, min_data_accuracy = minimum_data_for_accuracy(train_data, custom_classifier)\n","\n","results = {\n","    'Pretrained Model Accuracy': pretrained_accuracy,\n","    'Custom Model Accuracy': custom_model_accuracy,\n","    'Minimum Data Needed for Target Accuracy': min_data_needed,\n","    'Achieved Accuracy with Minimum Data': min_data_accuracy\n","}\n","\n","print(results)\n"]},{"cell_type":"markdown","metadata":{"id":"KUu1AprC-YKc"},"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOAERhyFBuaIqeTUmpjGYo+","mount_file_id":"1YNmZS3h-2z-RzQMU1-Vq0PpvGdU3nrZM","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"05024c37b963495281504e6f438a4cfa":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"164a65d73e3c45819aca68dba7807d14":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"179dd406572046b98077e0e9f6c01852":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d15cce4e76704d41b100c8c95e942746","IPY_MODEL_ca787cf3bdbf4eecb65d8980897c78c4","IPY_MODEL_7f37237414ee45dd89c67bbf764a6778"],"layout":"IPY_MODEL_4f815e8b3d8249afb0eecfd34dc9d731"}},"1acd4a01bc9a4d7ba2513f8c856dd580":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a550fdeac51e46baa86988e99123fac0","placeholder":"​","style":"IPY_MODEL_a8da103e041e44339058c2ad0d200e6c","value":"Map: 100%"}},"1f06c6b7ea014844b11a3426eb86235c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"45457146328743bc996f175614a1d9b5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"48396c6c620e49f5b4aed02682c89180":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1acd4a01bc9a4d7ba2513f8c856dd580","IPY_MODEL_5bde8918473e4505abdfa3484ea03302","IPY_MODEL_dab6a71238d44d2ca115961a15197c94"],"layout":"IPY_MODEL_05024c37b963495281504e6f438a4cfa"}},"4f815e8b3d8249afb0eecfd34dc9d731":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5bde8918473e4505abdfa3484ea03302":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6fa736c39c0343f789db6adf5ab6bad9","max":2214,"min":0,"orientation":"horizontal","style":"IPY_MODEL_689ae685e49a4e719c90164b5e96ab02","value":2214}},"689ae685e49a4e719c90164b5e96ab02":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6cd93d0c5c904014942d82248675bf99":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6fa736c39c0343f789db6adf5ab6bad9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f37237414ee45dd89c67bbf764a6778":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c5e69014299944cd8f9294ce02d0c583","placeholder":"​","style":"IPY_MODEL_45457146328743bc996f175614a1d9b5","value":" 554/554 [00:01\u0026lt;00:00, 421.35 examples/s]"}},"a550fdeac51e46baa86988e99123fac0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a8da103e041e44339058c2ad0d200e6c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aae3c6dd93af42ed84d0e95ee35fc0c5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c5e69014299944cd8f9294ce02d0c583":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca787cf3bdbf4eecb65d8980897c78c4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e083dc5098664a04a812d073d414dec6","max":554,"min":0,"orientation":"horizontal","style":"IPY_MODEL_164a65d73e3c45819aca68dba7807d14","value":554}},"d15cce4e76704d41b100c8c95e942746":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f61e5a32e45c40f3b2a454f4f94989d3","placeholder":"​","style":"IPY_MODEL_1f06c6b7ea014844b11a3426eb86235c","value":"Map: 100%"}},"dab6a71238d44d2ca115961a15197c94":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aae3c6dd93af42ed84d0e95ee35fc0c5","placeholder":"​","style":"IPY_MODEL_6cd93d0c5c904014942d82248675bf99","value":" 2214/2214 [00:03\u0026lt;00:00, 588.10 examples/s]"}},"e083dc5098664a04a812d073d414dec6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f61e5a32e45c40f3b2a454f4f94989d3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}