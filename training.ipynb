{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h_iDFMMDvis4","executionInfo":{"status":"ok","timestamp":1723540311726,"user_tz":-345,"elapsed":24591,"user":{"displayName":"Sakshi Nepal","userId":"06409844641326949268"}},"outputId":"8880f206-34fc-4ed2-f50e-5b95f16c9ce2"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n","\u001b[0mCollecting unsloth@ git+https://github.com/unslothai/unsloth.git (from unsloth[colab]@ git+https://github.com/unslothai/unsloth.git)\n","  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-install-7d972agv/unsloth_e8d1eb598f5442c3873984f3d6fcd9e8\n","  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-install-7d972agv/unsloth_e8d1eb598f5442c3873984f3d6fcd9e8\n","  Resolved https://github.com/unslothai/unsloth.git to commit 3781a03903c6a24c929737f49a1f73b25a517ac6\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.23.5)\n","Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (7.34.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.15.4)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.5.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.5)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython) (72.1.0)\n","Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython) (0.19.1)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython) (0.7.5)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython) (5.7.1)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython) (3.0.47)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython) (2.16.1)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython) (0.2.0)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython) (0.1.7)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython) (4.9.0)\n","Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython) (0.8.4)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython) (0.2.13)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.7.4)\n","Requirement already satisfied: bitsandbytes>=0.43.3 in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab]@ git+https://github.com/unslothai/unsloth.git) (0.43.3)\n","Requirement already satisfied: tyro in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab]@ git+https://github.com/unslothai/unsloth.git) (0.8.6)\n","Requirement already satisfied: transformers>=4.43.2 in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab]@ git+https://github.com/unslothai/unsloth.git) (4.44.0)\n","Requirement already satisfied: datasets>=2.16.0 in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab]@ git+https://github.com/unslothai/unsloth.git) (2.20.0)\n","Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab]@ git+https://github.com/unslothai/unsloth.git) (0.2.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab]@ git+https://github.com/unslothai/unsloth.git) (5.9.5)\n","Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab]@ git+https://github.com/unslothai/unsloth.git) (0.44.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab]@ git+https://github.com/unslothai/unsloth.git) (1.26.4)\n","Requirement already satisfied: accelerate>=0.26.1 in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab]@ git+https://github.com/unslothai/unsloth.git) (0.32.1)\n","Requirement already satisfied: trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab]@ git+https://github.com/unslothai/unsloth.git) (0.8.6)\n","Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab]@ git+https://github.com/unslothai/unsloth.git) (0.12.0)\n","Requirement already satisfied: protobuf<4.0.0 in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab]@ git+https://github.com/unslothai/unsloth.git) (3.20.3)\n","Requirement already satisfied: hf-transfer in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab]@ git+https://github.com/unslothai/unsloth.git) (0.1.8)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab]@ git+https://github.com/unslothai/unsloth.git) (2.1.0)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab]@ git+https://github.com/unslothai/unsloth.git) (0.4.4)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab]@ git+https://github.com/unslothai/unsloth.git) (17.0.0)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab]@ git+https://github.com/unslothai/unsloth.git) (0.6)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab]@ git+https://github.com/unslothai/unsloth.git) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab]@ git+https://github.com/unslothai/unsloth.git) (2.2.2)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab]@ git+https://github.com/unslothai/unsloth.git) (3.4.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab]@ git+https://github.com/unslothai/unsloth.git) (0.70.16)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab]@ git+https://github.com/unslothai/unsloth.git) (3.10.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.43.2->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab]@ git+https://github.com/unslothai/unsloth.git) (2024.5.15)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.43.2->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab]@ git+https://github.com/unslothai/unsloth.git) (0.19.1)\n","Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab]@ git+https://github.com/unslothai/unsloth.git) (0.16)\n","Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab]@ git+https://github.com/unslothai/unsloth.git) (13.7.1)\n","Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab]@ git+https://github.com/unslothai/unsloth.git) (1.7.1)\n","Collecting xformers@ https://download.pytorch.org/whl/cu121/xformers-0.0.22.post7-cp310-cp310-manylinux2014_x86_64.whl (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab]@ git+https://github.com/unslothai/unsloth.git)\n","  Using cached https://download.pytorch.org/whl/cu121/xformers-0.0.22.post7-cp310-cp310-manylinux2014_x86_64.whl (211.8 MB)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab]@ git+https://github.com/unslothai/unsloth.git) (2.3.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab]@ git+https://github.com/unslothai/unsloth.git) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab]@ git+https://github.com/unslothai/unsloth.git) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab]@ git+https://github.com/unslothai/unsloth.git) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab]@ git+https://github.com/unslothai/unsloth.git) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab]@ git+https://github.com/unslothai/unsloth.git) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab]@ git+https://github.com/unslothai/unsloth.git) (4.0.3)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab]@ git+https://github.com/unslothai/unsloth.git) (3.0.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.26.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab]@ git+https://github.com/unslothai/unsloth.git) (1.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.26.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab]@ git+https://github.com/unslothai/unsloth.git) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.26.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab]@ git+https://github.com/unslothai/unsloth.git) (3.1.4)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.26.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab]@ git+https://github.com/unslothai/unsloth.git) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.26.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab]@ git+https://github.com/unslothai/unsloth.git) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.26.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab]@ git+https://github.com/unslothai/unsloth.git) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.26.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab]@ git+https://github.com/unslothai/unsloth.git) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.26.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab]@ git+https://github.com/unslothai/unsloth.git) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.26.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab]@ git+https://github.com/unslothai/unsloth.git) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.26.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab]@ git+https://github.com/unslothai/unsloth.git) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.26.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab]@ git+https://github.com/unslothai/unsloth.git) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.26.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab]@ git+https://github.com/unslothai/unsloth.git) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.26.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab]@ git+https://github.com/unslothai/unsloth.git) (2.18.1)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.26.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab]@ git+https://github.com/unslothai/unsloth.git) (12.1.105)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.26.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab]@ git+https://github.com/unslothai/unsloth.git) (2.1.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate>=0.26.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab]@ git+https://github.com/unslothai/unsloth.git) (12.6.20)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab]@ git+https://github.com/unslothai/unsloth.git) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab]@ git+https://github.com/unslothai/unsloth.git) (2024.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab]@ git+https://github.com/unslothai/unsloth.git) (2024.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab]@ git+https://github.com/unslothai/unsloth.git) (0.1.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab]@ git+https://github.com/unslothai/unsloth.git) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate>=0.26.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab]@ git+https://github.com/unslothai/unsloth.git) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate>=0.26.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab]@ git+https://github.com/unslothai/unsloth.git) (1.3.0)\n","\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["!pip install huggingface_hub ipython \"unsloth[colab] @ git+https://github.com/unslothai/unsloth.git\" \"unsloth[conda] @ git+https://github.com/unslothai/unsloth.git\"\n"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":593,"referenced_widgets":["4de5e86ee89541c894fbc437f80e2e1b","1d14e0cf1d5c4caf9fa548b2bd9d7a44","4078d6b2337141e98daa6c9ad9022525","3bf5078c9c004b8498710a3f5209ade2","53ed154a22804b19b48c23978c091701","06b4a5d90e504b769eaf40812a8a7fe0","78221db1be8c4c49b6d9d1f3ad390dbb","a31eb0b94b1e4920b4d5974d028771c7","ed103c8e60d24ae0aa97d5ea1ae215c1","1ea004dc1cec4f39befb57252b511aac","8ffcebaff47c4237b50c937d6ddd4809"]},"executionInfo":{"elapsed":7067,"status":"error","timestamp":1723541883735,"user":{"displayName":"Sakshi Nepal","userId":"06409844641326949268"},"user_tz":-345},"id":"HPK4TE0U30Rp","outputId":"b710e067-347f-4b17-e27a-f28f5469091c"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/2792 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4de5e86ee89541c894fbc437f80e2e1b"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["==((====))==  Unsloth 2024.8: Fast Llama patching. Transformers = 4.44.0.\n","   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n","O^O/ \\_/ \\    Pytorch: 2.1.0+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n","\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.22.post7. FA2 = False]\n"," \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"]},{"output_type":"error","ename":"TypeError","evalue":"LlamaForCausalLM.__init__() got an unexpected keyword argument 'load_in_8bit_fp32_cpu_offload'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-178d6aabb137>\u001b[0m in \u001b[0;36m<cell line: 55>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m# Load the model and tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m# Load the model and tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m model, tokenizer = FastLanguageModel.from_pretrained(\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsloth/llama-3-8b-bnb-4bit\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/unsloth/models/loader.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, *args, **kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m         model, tokenizer = dispatch_model.from_pretrained(\n\u001b[0m\u001b[1;32m    273\u001b[0m             \u001b[0mmodel_name\u001b[0m        \u001b[0;34m=\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0mmax_seq_length\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/unsloth/models/llama.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, model_patcher, tokenizer_name, trust_remote_code, **kwargs)\u001b[0m\n\u001b[1;32m   1374\u001b[0m         \u001b[0mmax_position_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_max_seq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"attn_implementation\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0;31m# No need since we auto call it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1376\u001b[0;31m         model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m   1377\u001b[0m             \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m             \u001b[0mdevice_map\u001b[0m              \u001b[0;34m=\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    565\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3808\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mContextManagers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_contexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3809\u001b[0m             \u001b[0;31m# Let's make sure we don't run the init function of buffer modules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3810\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3812\u001b[0m         \u001b[0;31m# If we init with `zero3`, add an attr to the model so we can check downstream for issues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: LlamaForCausalLM.__init__() got an unexpected keyword argument 'load_in_8bit_fp32_cpu_offload'"]}],"source":["\n","import torch\n","import os\n","import pandas as pd\n","from transformers import TextStreamer, AutoTokenizer\n","from datasets import Dataset\n","from trl import SFTTrainer\n","from transformers import TrainingArguments\n","from unsloth import FastLanguageModel, is_bfloat16_supported\n","from google.colab import drive\n","import time\n","\n","# Load your custom dataset\n","\n","\n","# Load your custom dataset from Google Drive\n","data_path = 'Copy_preprossed_datasets - preprossed_datasets.csv.csv'\n","df = pd.read_csv(data_path)\n","\n","# Filter rows where the value is true\n","df_true = df\n","EOS_TOKEN = tokenizer.eos_token\n","# Define the formatting function for the filtered dataset\n","def formatting_prompts_func(examples):\n","    instructions = examples[\"user query\"]\n","    inputs       = examples[\"assistant responses\"]\n","    outputs      = examples[\"is_liked\"]\n","    texts = []\n","    for instruction, input, output in zip(instructions, inputs, outputs):\n","        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n","        texts.append(text)\n","    return {\"text\": texts}\n","\n","# Convert to Hugging Face Dataset\n","dataset = Dataset.from_pandas(df_true)\n","dataset = dataset.map(formatting_prompts_func, batched=True)\n","\n","# Configuration\n","max_seq_length = 2048\n","dtype = None\n","load_in_4bit = True\n","alpaca_prompt = \"\"\"Below is a user query followed by an assistant's response. Your task is to generate a response with user liked feedback that best matches the assistant's tone, context, and accuracy and avoid generating users not liked responses.\n","\n","### User Query:\n","{}\n","\n","### Assistant Response:\n","{}\n","\n","### Generate a response that continues the assistant's approach:\"\"\"\n","\n","huggingface_model_name = \"sakshi-rumsan/Llama-3.1-8B-bnb-4bit-python\"\n","\n","# Load the model and tokenizer\n","# Load the model and tokenizer\n","model, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name=\"unsloth/llama-3-8b-bnb-4bit\",\n","    max_seq_length=max_seq_length,\n","    dtype=dtype,\n","    load_in_4bit=load_in_4bit,\n","    token=os.getenv(\"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\"),\n","    load_in_8bit_fp32_cpu_offload=True, # Enable CPU offloading\n","    device_map=\"auto\", # Let Transformers automatically decide device placement\n",")\n","model = FastLanguageModel.for_inference(model, load_in_8bit_fp32_cpu_offload=True)\n","\n","# Enable native 2x faster inference before training\n","\n","\n","# Start the training\n","start_time = time.time()\n","\n","trainer = SFTTrainer(\n","    model=model,\n","    tokenizer=tokenizer,\n","    train_dataset=dataset,\n","    dataset_text_field=\"text\",\n","    max_seq_length=max_seq_length,\n","    dataset_num_proc=2,\n","    packing=False,\n","    args=TrainingArguments(\n","        per_device_train_batch_size=2,\n","        gradient_accumulation_steps=4,\n","        warmup_steps=5,\n","        max_steps=100,\n","        learning_rate=2e-4,\n","        fp16=not is_bfloat16_supported(),\n","        bf16=is_bfloat16_supported(),\n","        logging_steps=1,\n","        optim=\"adamw_8bit\",\n","        weight_decay=0.01,\n","        lr_scheduler_type=\"linear\",\n","        seed=3407,\n","        output_dir=\"outputs\",\n","    ),\n",")\n","\n","trainer.train()\n","\n","# Record the time taken for training\n","train_time = time.time() - start_time\n","print(f\"Training took {train_time:.2f} seconds.\")\n","\n","# After Training: Enable inference\n","FastLanguageModel.for_inference(model)\n","\n","# Prepare a test input\n","instruction = \"Write a detailed answer\"\n","input = \"What is the Product Specification ID in MEF 81?\"\n","inputs = tokenizer(\n","    [alpaca_prompt.format(instruction, input, \"\")],\n","    return_tensors=\"pt\"\n",").to(\"cuda\")\n","\n","# Generate and display the response\n","text_streamer = TextStreamer(tokenizer)\n","response_start_time = time.time()\n","_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=1000)\n","response_time = time.time() - response_start_time\n","\n","print(f\"Response generation took {response_time:.2f} seconds.\")\n","\n","# Save the fine-tuned model\n","model.save_pretrained(\"lora_model\")\n","tokenizer.save_pretrained(\"lora_model\")\n","model.push_to_hub(huggingface_model_name, token=os.getenv(\"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\"))\n","tokenizer.push_to_hub(huggingface_model_name, token=os.getenv(\"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\"))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jzAzlyRb0OrN"},"outputs":[],"source":["from unsloth import FastLanguageModel\n","import torch\n","import os\n","import pandas as pd\n","from transformers import TextStreamer\n","from datasets import Dataset\n","from trl import SFTTrainer\n","from transformers import TrainingArguments\n","from unsloth import is_bfloat16_supported\n","from google.colab import drive\n","\n","\n","\n","# Load your custom dataset\n","drive.mount('/content/drive')\n","EOS_TOKEN = tokenizer.eos_token\n","\n","# 2. Load your custom dataset from Google Drive\n","data_path = '/content/drive/MyDrive/datasets/Copy_preprossed_datasets.csv'  # Update the path\n","df = pd.read_csv(data_path)\n","\n","# Filter rows where the value is true\n","df_true = df[df['true'] == 1]  # Assuming the column 'true' exists in your CSV\n","\n","# Define the formatting function for the filtered dataset\n","def formatting_prompts_func(examples):\n","    instructions = examples[\"user query\"]\n","    inputs       = examples[\"assistant responses\"]\n","    outputs      = examples[\"is_liked\"]\n","    texts = []\n","    for instruction, input, output in zip(instructions, inputs, outputs):\n","        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n","        texts.append(text)\n","    return { \"text\" : texts }\n","\n","# Convert to Hugging Face Dataset\n","dataset = Dataset.from_pandas(df_true)\n","dataset = dataset.map(formatting_prompts_func, batched=True)\n","\n","# 1. Configuration\n","max_seq_length = 2048\n","dtype = None\n","load_in_4bit = True\n","alpaca_prompt =\"\"\"Below is a user query followed by an assistant's response. Your task is to generate a response with user liked feedback that best matches the assistant's tone, context, and accuracy and avoid generating users not liked responses.\n","\n","### User Query:\n","{}\n","\n","### Assistant Response:\n","{}\n","\n","### Generate a response that continues the assistant's approach:\"\"\"\n","\n","instruction = \"Write a detailed answer\"\n","input = \"What is the Product Specification ID in MEF 81?\"\n","huggingface_model_name = \"sakshi-rumsan/Llama-3.1-8B-bnb-4bit-python\"\n","\n","# 2. Before Training\n","model, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name=\"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",\n","    max_seq_length=max_seq_length,\n","    dtype=dtype,\n","    load_in_4bit=load_in_4bit,\n","    token=os.getenv(\"HF_TOKEN\")\n",")\n","\n","FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n","\n","# Tokenize the inputs\n","inputs = tokenizer(\n","    [\n","        alpaca_prompt.format(\n","            instruction,  # instruction\n","            input,  # input\n","            \"\",  # output - leave this blank for generation!\n","        )\n","    ], return_tensors=\"pt\").to(\"cuda\")\n","\n","text_streamer = TextStreamer(tokenizer)\n","_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=1000)\n","\n","# 4. Training\n","model = FastLanguageModel.get_peft_model(\n","    model,\n","    r=16,  # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n","    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n","                    \"gate_proj\", \"up_proj\", \"down_proj\", ],\n","    lora_alpha=16,\n","    lora_dropout=0,  # Supports any, but = 0 is optimized\n","    bias=\"none\",  # Supports any, but = \"none\" is optimized\n","    use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for very long context\n","    random_state=3407,\n","    use_rslora=False,  # We support rank stabilized LoRA\n","    loftq_config=None,  # And LoftQ\n",")\n","\n","trainer = SFTTrainer(\n","    model=model,\n","    tokenizer=tokenizer,\n","    train_dataset=dataset,\n","    dataset_text_field=\"text\",\n","    max_seq_length=max_seq_length,\n","    dataset_num_proc=2,\n","    packing=False,  # Can make training 5x faster for short sequences.\n","    args=TrainingArguments(\n","        per_device_train_batch_size=2,\n","        gradient_accumulation_steps=4,\n","        warmup_steps=5,\n","        max_steps=100,\n","        learning_rate=2e-4,\n","        fp16=not is_bfloat16_supported(),\n","        bf16=is_bfloat16_supported(),\n","        logging_steps=1,\n","        optim=\"adamw_8bit\",\n","        weight_decay=0.01,\n","        lr_scheduler_type=\"linear\",\n","        seed=3407,\n","        output_dir=\"outputs\",\n","    ),\n",")\n","\n","# Show current memory stats\n","gpu_stats = torch.cuda.get_device_properties(0)\n","start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n","max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n","print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n","print(f\"{start_gpu_memory} GB of memory reserved.\")\n","\n","trainer_stats = trainer.train()\n","\n","# Show final memory and time stats\n","used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n","used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n","used_percentage = round(used_memory / max_memory * 100, 3)\n","lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n","print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n","print(f\"{round(trainer_stats.metrics['train_runtime'] / 60, 2)} minutes used for training.\")\n","print(f\"Peak reserved memory = {used_memory} GB.\")\n","print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n","print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n","print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")\n","\n","# 5. After Training\n","FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n","\n","inputs = tokenizer(\n","    [\n","        alpaca_prompt.format(\n","            instruction,  # instruction\n","            input,  # input\n","            \"\",  # output - leave this blank for generation!\n","        )\n","    ], return_tensors=\"pt\").to(\"cuda\")\n","\n","text_streamer = TextStreamer(tokenizer)\n","_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=1000)\n","\n","# 6. Saving\n","model.save_pretrained(\"lora_model\")  # Local saving\n","tokenizer.save_pretrained(\"lora_model\")\n","model.push_to_hub(huggingface_model_name, token=os.getenv(\"HF_TOKEN\"))\n","tokenizer.push_to_hub(huggingface_model_name, token=os.getenv(\"HF_TOKEN\"))\n","\n","# Merge to 16bit\n","if True:\n","    model.save_pretrained_merged(\"model\", tokenizer, save_method=\"merged_16bit\",)\n","if True:\n","    model.push_to_hub_merged(huggingface_model_name, tokenizer, save_method=\"merged_16bit\", token=os.getenv(\"HF_TOKEN\"))\n","\n","# Convert to GGUF\n","model.push_to_hub_gguf(\n","    huggingface_model_name,  # Change this to your username!\n","    tokenizer,\n","    quantization_method=[\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n","    token=os.getenv(\"HF_TOKEN\"),\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"juIgLBSu1mzj"},"outputs":[],"source":["from unsloth import FastLanguageModel\n","import os\n","model, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name = \"lora_model\",\n","    max_seq_length = 2048,\n","    dtype = None,\n","    load_in_4bit = True,\n",")\n","FastLanguageModel.for_inference(model)\n","\n","model.push_to_hub_gguf(\n","    \"sakshi-rumsan/Llama-3.1-8B-bnb-4bit-python\", # Change this to your username!\n","    tokenizer,\n","    quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n","    token = os.getenv(\"HF_TOKEN\"),\n",")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyPVOCw4e3JRJ9zMhoRkzWir"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"4de5e86ee89541c894fbc437f80e2e1b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1d14e0cf1d5c4caf9fa548b2bd9d7a44","IPY_MODEL_4078d6b2337141e98daa6c9ad9022525","IPY_MODEL_3bf5078c9c004b8498710a3f5209ade2"],"layout":"IPY_MODEL_53ed154a22804b19b48c23978c091701"}},"1d14e0cf1d5c4caf9fa548b2bd9d7a44":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_06b4a5d90e504b769eaf40812a8a7fe0","placeholder":"​","style":"IPY_MODEL_78221db1be8c4c49b6d9d1f3ad390dbb","value":"Map: 100%"}},"4078d6b2337141e98daa6c9ad9022525":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a31eb0b94b1e4920b4d5974d028771c7","max":2792,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ed103c8e60d24ae0aa97d5ea1ae215c1","value":2792}},"3bf5078c9c004b8498710a3f5209ade2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1ea004dc1cec4f39befb57252b511aac","placeholder":"​","style":"IPY_MODEL_8ffcebaff47c4237b50c937d6ddd4809","value":" 2792/2792 [00:00&lt;00:00, 16823.08 examples/s]"}},"53ed154a22804b19b48c23978c091701":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"06b4a5d90e504b769eaf40812a8a7fe0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"78221db1be8c4c49b6d9d1f3ad390dbb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a31eb0b94b1e4920b4d5974d028771c7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed103c8e60d24ae0aa97d5ea1ae215c1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1ea004dc1cec4f39befb57252b511aac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ffcebaff47c4237b50c937d6ddd4809":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}